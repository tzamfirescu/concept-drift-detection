{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from scipy.io.arff import loadarff \n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "from numpy import mean\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from random import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier, Perceptron, Ridge, Lasso, ElasticNet, SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_data(path, features_encoder, size_training, scaler): \n",
    "    dataframe = pd.read_csv(path)\n",
    "    #the label has to be the last column in the file!\n",
    "    labels = dataframe.iloc[: , -1]\n",
    "    features = dataframe.iloc[:, :-1]\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    labels = le.fit_transform(labels)\n",
    "    print(features.shape)\n",
    "    if(features_encoder is None):\n",
    "        features = features._get_numeric_data()\n",
    "    else:\n",
    "        cols = features.columns\n",
    "        print('categorical features', cols)\n",
    "        numerical_cols = features._get_numeric_data().columns\n",
    "        categorical_cols = list(set(cols) - set(numerical_cols))\n",
    "        \n",
    "        if(isinstance(features_encoder, OneHotEncoder)):\n",
    "            feature_arr = features_encoder.fit_transform(features[categorical_cols])\n",
    "            feature_labels =  features_encoder.get_feature_names(categorical_cols)\n",
    "            encoded_features = pd.DataFrame(feature_arr.toarray(), columns=feature_labels)\n",
    "            \n",
    "        if(isinstance(features_encoder, OrdinalEncoder)):\n",
    "            feature_arr = features_encoder.fit_transform(features[categorical_cols])\n",
    "            encoded_features = pd.DataFrame(feature_arr, columns=categorical_cols)\n",
    "            \n",
    "        if(isinstance(features_encoder, TargetEncoder)):\n",
    "            transform = features_encoder.fit_transform(features[categorical_cols].iloc[:size_training], labels[:size_training])\n",
    "            training_encoded = pd.DataFrame(transform, columns=categorical_cols)\n",
    "            testing_encoded = pd.DataFrame(features_encoder.transform(features[categorical_cols].iloc[size_training:len(features)]), columns=categorical_cols)\n",
    "            encoded_features = training_encoded.append(testing_encoded)\n",
    "            \n",
    "        features = features._get_numeric_data().join(encoded_features)\n",
    "        \n",
    "    if(scaler is True):\n",
    "        scaler = MinMaxScaler()\n",
    "        features_training = scaler.fit_transform(features.iloc[:size_training])\n",
    "        features_testing = scaler.transform(features.iloc[size_training:len(features)])\n",
    "        features_training_df = pd.DataFrame(features_training, columns=features.columns)\n",
    "        features_testing_df = pd.DataFrame(features_testing, columns=features.columns)\n",
    "        features = features_training_df.append(features_testing_df)\n",
    "        \n",
    "    return features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(features, labels, size_training, scaler): \n",
    "    training_features = features.iloc[:size_training]\n",
    "    training_labels = labels[:size_training]\n",
    "    return training_features, training_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_classifier(training_features, training_labels, classifier): \n",
    "    classifier.fit(training_features, training_labels)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "# def find_best_parameters_random_forest(training_features, training_labels):\n",
    "    \n",
    "#     # Number of trees in random forest\n",
    "#     n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 100)]\n",
    "#     # Maximum number of levels in tree\n",
    "#     max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "#     # Method of selecting samples for training each tree\n",
    "#     bootstrap = [True, False]\n",
    "#     # Create the random grid\n",
    "#     random_grid = {'n_estimators': n_estimators,\n",
    "#                    'max_depth': max_depth,\n",
    "#                    'bootstrap': bootstrap}\n",
    "#     #print(random_grid)\n",
    "#     rf = RandomForestClassifier()\n",
    "#     rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "#     # Fit the random search model\n",
    "#     rf_random.fit(training_features, training_labels)\n",
    "#     print('best parameters', rf_random.best_params_)\n",
    "#     return rf_random.best_estimator_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score_training_set(model, training_features, training_labels, length_test_set, type_splitting):\n",
    "    countEvents = len(training_features)\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    cv = []\n",
    "    number_runs = 20\n",
    "    accuracy_list = []\n",
    "    for i in range(number_runs):\n",
    "        #sequential splitting method\n",
    "        if(type_splitting == 0):\n",
    "            start = randint(0, countEvents - number_runs)\n",
    "            x_test = pd.DataFrame(training_features[start:start + length_test_set], columns = training_features.columns)\n",
    "            y_test = pd.DataFrame(training_labels[start:start + length_test_set])\n",
    "\n",
    "            x_train_before = pd.DataFrame(training_features[0:start], columns=training_features.columns)\n",
    "            x_train_after = pd.DataFrame(training_features[start + length_test_set:countEvents], columns = training_features.columns)\n",
    "            x_train = x_train_before.append(x_train_after)\n",
    "\n",
    "            y_train_before = pd.DataFrame(training_labels[0:start])\n",
    "            y_train_after = pd.DataFrame(training_labels[start + length_test_set:countEvents])\n",
    "            y_train = y_train_before.append(y_train_after)\n",
    "        \n",
    "        #time-based splitting\n",
    "        if(type_splitting == 1):\n",
    "            start_testing = randint(int((countEvents - number_runs) * 0.5), int((countEvents - number_runs) * 0.9))\n",
    "            x_test = pd.DataFrame(training_features[start_testing:], columns = training_features.columns)\n",
    "            y_test = pd.DataFrame(training_labels[start_testing:])\n",
    "\n",
    "            x_train = pd.DataFrame(training_features[0:start_testing], columns = training_features.columns)\n",
    "            y_train = pd.DataFrame(training_labels[0:start_testing])\n",
    "\n",
    "        results = model.fit(x_train, y_train)\n",
    "        predicted_labels = model.predict(x_test)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, predicted_labels)\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    training_mean = np.mean(accuracy_list)\n",
    "    training_std = np.std(accuracy_list)\n",
    "    return [training_mean, training_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_batches(classifier, features, labels, scores_training_set, size_training, size_dataset, size_batch):\n",
    "    detected_batches = []\n",
    "    detected = False\n",
    "    training_mean = scores_training_set[0]\n",
    "    training_std = scores_training_set[1]\n",
    "    batch_scores = []\n",
    "    total_batches = int((size_dataset - size_training) / size_batch)\n",
    "    print('training mean:', training_mean, ', training std:', training_std)\n",
    "    for i in range(size_training, size_dataset, size_batch):\n",
    "        batch = features[i:i + size_batch]\n",
    "        batch_labels = labels[i:i+size_batch]\n",
    "        batch_score = classifier.score(batch, batch_labels)\n",
    "        batch_scores.append(batch_score)\n",
    "        batch_number = int((i - size_training) / size_batch + 1)\n",
    "        if(training_mean - 1.00 * training_std > batch_score):\n",
    "            detected_batches.append(batch_number)\n",
    "    print('detected batches:', detected_batches)\n",
    "    \n",
    "    #draw graph with accuracies of each batch\n",
    "    batch_scores = np.array(batch_scores)\n",
    "    fig, ax = plt.subplots()\n",
    "    indices = np.arange(1, total_batches + 1)\n",
    "    plt.axhline(y = training_mean, label='mean', color='red')\n",
    "    plt.axhline(y = training_mean - 1.00 * training_std, label='mean - 1 std', color='grey')\n",
    "    plt.xlabel('Batch Number')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0.5,1)\n",
    "    colors = []\n",
    "    for score in batch_scores:\n",
    "        if(score >= training_mean - 1.00 * training_std):\n",
    "            colors.append('green')\n",
    "        else:\n",
    "            colors.append('red')\n",
    "    ax.bar(indices, batch_scores, color = colors)\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "    print()\n",
    "\n",
    "    return detected_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_drift(path, size_dataset, size_training, size_batch, features_encoder, scaler, type_splitting):\n",
    "    features, labels = get_all_data(path, features_encoder, size_training, scaler)\n",
    "    training_features, training_labels = get_training_data(features, labels, size_training, scaler)\n",
    "    #tried, bad for 'spam': svm.LinearSVC(), LogisticRegression(), PassiveAggressiveClassifier(),  Perceptron(), Lasso(), ElasticNet() SGDClassifier() MultinomialNB(), svm.SVR() ,  BernoulliNB(alpha=.001)\n",
    "    \n",
    "    #spam data set\n",
    "    models = [RandomForestClassifier(n_estimators = 1000, max_depth = 40, bootstrap = False)]\n",
    "#     model weather = [svm.SVC(random_state=42, C=2.09)]\n",
    "#     model elect2 and airline = [KNeighborsClassifier(n_neighbors=100)]\n",
    "    for model in models:\n",
    "        print('model:', model.__class__.__name__)\n",
    "        classifier = learn_classifier(training_features, training_labels, model)\n",
    "        scores_training_set = compute_score_training_set(classifier, training_features, training_labels, size_batch, type_splitting)\n",
    "        check_batches(classifier, features, labels, scores_training_set, size_training, size_dataset, size_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(path, size_dataset, size_training, size_batch, features_encoder, scaler, type_splitting):\n",
    "    start = time.time()\n",
    "    #adjust size of dataset so that all batches have equal size\n",
    "    size_dataset = size_training + int((size_dataset - size_training)/ size_batch) * size_batch\n",
    "    print('dataset:', path)\n",
    "    print('size dataset: ' + str(size_dataset) + ', size training: ' + str(size_training) + ', size testing batch: ' + str(size_batch))\n",
    "    if(features_encoder is not None):\n",
    "        print('categorical features encoder', features_encoder.__class__.__name__)\n",
    "    if(scaler is True):\n",
    "        print('features scaled using MinMaxScaler')\n",
    "    else:\n",
    "        print('features are not scaled')\n",
    "    if(type_splitting == 0):\n",
    "        print('sequential splitting method')\n",
    "    else:\n",
    "        print('time-based splitting method')\n",
    "    print()\n",
    "    predicted_batches = []  \n",
    "    number_testing_batches = (size_dataset - size_training) / size_batch\n",
    "    get_concept_drift(path, size_dataset, size_training, size_batch, features_encoder, scaler, type_splitting)\n",
    "    end = time.time()\n",
    "    print(\"duration of test: \" + str(int((end - start) / 60)) + ' minutes')\n",
    "    print()\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPAM\n",
    "#RandomForestClassifier(n_estimators = 1000, max_depth = 40, bootstrap = False)\n",
    "\n",
    "#size batch is 100, sequential splitting method (last parameter = 0)\n",
    "run_test('real-world/spam_dataset.csv', 4405, 1468, 100, None, False, 0)\n",
    "#size batch is 100, time-based splitting method (last parameter = 1)\n",
    "run_test('real-world/spam_dataset.csv', 4405, 1468, 100, None, False, 1)\n",
    "\n",
    "#size batch is 50, sequential splitting method\n",
    "run_test('real-world/spam_dataset.csv', 4405, 1468, 50, None, False, 0)\n",
    "#size batch is 50, time-based splitting method\n",
    "run_test('real-world/spam_dataset.csv', 4405, 1468, 50, None, False, 1)\n",
    "\n",
    "#size batch is 20, sequential splitting method\n",
    "run_test('real-world/spam_dataset.csv', 4405, 1468, 20, None, False, 0)\n",
    "#size batch is 20, time-based splitting method\n",
    "run_test('real-world/spam_dataset.csv', 4405, 1468, 20, None, False, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ELECT2\n",
    "#KNeighborsClassifier(n_neighbors=100)\n",
    "\n",
    "#size batch  365, sequential splitting method\n",
    "run_test('real-world/electricity_dataset.csv', 45312, 15104, 365, None, True, 0)\n",
    "#size batch is 365, time-based splitting method\n",
    "run_test('real-world/electricity_dataset.csv', 45312, 15104, 365, None, True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #WEATHER\n",
    "# # SVC(random_state=random_state, C=2.09)\n",
    "\n",
    "#size batch is 365, sequential splitting method \n",
    "run_test('real-world/weather_dataset.csv', 18159, 6053, 365, None, True, 0)\n",
    "#size batch is 365, time-based splitting method \n",
    "run_test('real-world/weather_dataset.csv', 18159, 6053, 365, None, True, 1)\n",
    "\n",
    "#size batch is 30, sequential splitting method \n",
    "run_test('real-world/weather_dataset.csv', 18159, 6053, 30, None, True, 0)\n",
    "#size batch is 30, time-based splitting method \n",
    "run_test('real-world/weather_dataset.csv', 18159, 6053, 30, None, True, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #AIRLINE\n",
    "#KNeighborsClassifier(n_neighbors=100)\n",
    "\n",
    "# #size batch is 17000, sequential splitting method \n",
    "run_test('real-world/airline_dataset.csv', 539383, 179794, 17000, OneHotEncoder(), True, 0)\n",
    "# #size batch is 17000, time-based splitting method \n",
    "run_test('real-world/airline_dataset.csv', 539383, 179794, 17000, OneHotEncoder(), True, 1)\n",
    "\n",
    "# #size batch is 17000, sequential splitting method \n",
    "run_test('real-world/airline_dataset.csv', 539383, 179794, 17000, OrdinalEncoder(), True, 0)\n",
    "# #size batch is 17000, time-based splitting method \n",
    "run_test('real-world/airline_dataset.csv', 539383, 179794, 17000, OrdinalEncoder(), True, 1)\n",
    "\n",
    "# #size batch is 17000, sequential splitting method \n",
    "run_test('real-world/airline_dataset.csv', 539383, 179794, 17000, TargetEncoder(), True, 0)\n",
    "# #size batch is 17000, time-based splitting method \n",
    "run_test('real-world/airline_dataset.csv', 539383, 179794, 17000, TargetEncoder(), True, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
