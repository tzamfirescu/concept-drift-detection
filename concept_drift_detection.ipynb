{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "from scipy.stats import ks_2samp\n",
    "import pandas as pd\n",
    "from scipy.io.arff import loadarff \n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "from numpy import mean\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer, StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from river import drift\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path, label):\n",
    "    dataframe = pd.DataFrame()\n",
    "    if 'synthetic' in path:\n",
    "        data = loadarff(path)\n",
    "        dataframe = pd.DataFrame(data[0])\n",
    "    if 'real-world' in path:\n",
    "        dataframe = pd.read_csv(path)\n",
    "        \n",
    "    labels = dataframe.loc[:, dataframe.columns == label]\n",
    "    features = dataframe.loc[:, dataframe.columns != label]\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    labels = le.fit_transform(labels)\n",
    "    return features, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(features, training_labels, size_training, features_encoder, features_scaler): \n",
    "    if(features_encoder is None):\n",
    "        features = features._get_numeric_data()\n",
    "    else:\n",
    "        cols = features.columns\n",
    "        numerical_cols = features._get_numeric_data().columns\n",
    "        categorical_cols = list(set(cols) - set(numerical_cols))\n",
    "        \n",
    "        if(isinstance(features_encoder, OneHotEncoder)):\n",
    "            feature_arr = features_encoder.fit_transform(features[categorical_cols])\n",
    "            feature_labels =  features_encoder.get_feature_names(categorical_cols)\n",
    "            encoded_features = pd.DataFrame(feature_arr.toarray(), columns=feature_labels)\n",
    "            \n",
    "        if(isinstance(features_encoder, OrdinalEncoder)):\n",
    "            feature_arr = features_encoder.fit_transform(features[categorical_cols])\n",
    "            encoded_features = pd.DataFrame(feature_arr, columns=categorical_cols)\n",
    "            \n",
    "        if(isinstance(features_encoder, TargetEncoder)):\n",
    "            transform = features_encoder.fit_transform(features[categorical_cols].iloc[:size_training], training_labels)\n",
    "            training_encoded = pd.DataFrame(transform, columns=categorical_cols)\n",
    "            testing_encoded = pd.DataFrame(features_encoder.transform(features[categorical_cols].iloc[size_training:len(features)]), columns=categorical_cols)\n",
    "            encoded_features = training_encoded.append(testing_encoded)\n",
    "            \n",
    "        features = features._get_numeric_data().join(encoded_features)\n",
    "         \n",
    "    if(features_scaler is True):\n",
    "        scaler = MinMaxScaler()\n",
    "        features_training = scaler.fit_transform(features.iloc[:size_training])\n",
    "        features_testing = scaler.transform(features.iloc[size_training:len(features)])\n",
    "        features_training_df = pd.DataFrame(features_training, columns=features.columns)\n",
    "        features_testing_df = pd.DataFrame(features_testing, columns=features.columns)\n",
    "        features = features_training_df.append(features_testing_df)\n",
    "        \n",
    "    if(features_encoder is not None):\n",
    "        print('categorical features encoder', features_encoder.__class__.__name__)\n",
    "    else:\n",
    "        print('no features encoder')\n",
    "    if(features_scaler is True):\n",
    "        print('features scaled using MinMaxScaler')\n",
    "    else:\n",
    "        print('features are not scaled')\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(features, labels, size_training): \n",
    "    training_features = features.iloc[:size_training]\n",
    "    training_labels = labels[:size_training]\n",
    "    return [training_features, training_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_classifier(training_features, training_labels):\n",
    "    classifier = RandomForestClassifier(n_estimators = 100)\n",
    "#     classifier = svm.SVC(probability=True)\n",
    "    classifier.fit(training_features, training_labels)\n",
    "#     folds = range(5,31, 1)\n",
    "#     #evalcrossvaluation\n",
    "#     # evaluate each k value\n",
    "#     for k in folds:\n",
    "#     # define the test condition\n",
    "#         cv = KFold(n_splits=k, shuffle=True, random_state=10)\n",
    "#         # record mean and min/max of each set of results\n",
    "#         k_mean, k_min, k_max = evaluate_model(cv,training_features,training_labels, classifier)\n",
    "#         # report performance\n",
    "#         print('-> folds=%d, accuracy=%.3f (%.3f,%.3f)' % (k, k_mean, k_min, k_max))\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "def evaluate_model(cv, X, y, model):\n",
    "    # evaluate the model\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    # return scores\n",
    "    return mean(scores), scores.min(), scores.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drift Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_summary(features):\n",
    "    centroid = features.sum(axis=0) / len(features)\n",
    "    sum_differences = 0\n",
    "    features = features.values    \n",
    "    for feature in features:\n",
    "        sum_differences = sum_differences + np.linalg.norm(feature - centroid.values)\n",
    "    mean_E_d = sum_differences / (len(features) - 1)\n",
    "    \n",
    "    return mean_E_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_parameter(summary_arr):\n",
    "    training_mean = np.mean(summary_arr)\n",
    "    std_dev = np.std(summary_arr)\n",
    "    number_subgroups = len(summary_arr)\n",
    "    R_sum = 0\n",
    "    for i in range(number_subgroups - 1):\n",
    "        R_sum = R_sum + np.linalg.norm(summary_arr[i + 1] - summary_arr[i])\n",
    "    R_mean = R_sum / (number_subgroups - 1)\n",
    "    A2 = 0.1\n",
    "    #print(R_mean)\n",
    "    #subgroup = 25\n",
    "    LCL = training_mean -  A2 * R_mean\n",
    "    UCL = training_mean +  A2 * R_mean\n",
    "    print('lcl, ucl', LCL, UCL)\n",
    "    return training_mean, UCL, LCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def udetect_all_batches(training_data, testing_features, size_batch, encoder, scaler):\n",
    "    training_features = training_data[0]\n",
    "    training_labels = training_data[1]\n",
    "    size_training = len(training_features)\n",
    "    all_features = training_features.append(testing_features)\n",
    "    all_features_ready = prepare_features(all_features, training_labels, size_training, encoder, scaler)\n",
    "    training_features = all_features_ready.iloc[0:size_training, :]\n",
    "    testing_features = all_features_ready.iloc[size_training:,]\n",
    "    \n",
    "    detected_batches = []\n",
    "    detected = False\n",
    "    \n",
    "\n",
    "    size_subgroup = 25\n",
    "    training_summaries = []\n",
    "    for i in range(0, size_training, size_subgroup):\n",
    "        training_summaries.append(window_summary(training_features.iloc[i:i + size_subgroup]))\n",
    "    training_mean, UCL_Ed, LCL_Ed = change_parameter(training_summaries)    \n",
    "    testing_summaries = []\n",
    "    for i in range(0, len(testing_features), size_batch):\n",
    "        batch_number = int(i / size_batch) + 1\n",
    "        testing_summaries_batch = []\n",
    "        for j in range(i, i + size_batch, size_subgroup):\n",
    "            testing_summaries_batch.append(window_summary(testing_features.iloc[j:j + size_subgroup]))\n",
    "        batch_mean = np.mean(testing_summaries_batch)\n",
    "        testing_summaries.append(np.mean(testing_summaries_batch))\n",
    "    total_batches = int(len(testing_features) / size_batch)\n",
    "    for i in range(len(testing_summaries)):\n",
    "        batch_number = i + 1\n",
    "        summary = testing_summaries[i]\n",
    "        if(summary < LCL_Ed or summary > UCL_Ed):\n",
    "            detected_batches.append(batch_number)\n",
    "            if (detected is False):\n",
    "                print('detected!')\n",
    "                detected = True\n",
    "            \n",
    "    if (detected is False):\n",
    "        print('no concept drift detected')\n",
    "    print('in batches:', detected_batches)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    indices = np.arange(1, total_batches + 1)\n",
    "    plt.axhline(y = UCL_Ed, label='UCL', color='red')\n",
    "    plt.axhline(y = training_mean, label='training mean', color='green')\n",
    "    plt.axhline(y = LCL_Ed, label='LCL', color='yellow')\n",
    "    colors = []\n",
    "    for summary in testing_summaries:\n",
    "        if(summary < LCL_Ed or summary > UCL_Ed):\n",
    "            colors.append('red')\n",
    "        else:\n",
    "            colors.append('green')\n",
    "    ax.bar(indices, np.array(testing_summaries), color=colors)\n",
    "    plt.ylim(0.9 * LCL_Ed, 1.1 * UCL_Ed)\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "    print()\n",
    "    \n",
    "    return detected_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def udetect_one_batch(training_data, testing_features, size_batch, encoder, scaler):\n",
    "    training_features = training_data[0]\n",
    "    training_labels = training_data[1]\n",
    "    size_training = len(training_features)\n",
    "    all_features = training_features.append(testing_features)\n",
    "    all_features_ready = prepare_features(all_features, training_labels, size_training, encoder, scaler)\n",
    "    training_features = all_features_ready.iloc[0:size_training, :]\n",
    "    testing_features = all_features_ready.iloc[size_training:,]\n",
    "    \n",
    "    detected_batches = []\n",
    "    detected = False\n",
    "\n",
    "    training_summaries = []\n",
    "    for i in range(0, size_training, size_subgroup):\n",
    "        training_summaries.append(window_summary(training_features.iloc[i:i + size_subgroup]))\n",
    "    training_mean, UCL_Ed, LCL_Ed = change_parameter(training_summaries)    \n",
    "\n",
    "    testing_summaries = []\n",
    "    for i in range(0, len(testing_features), size_batch):\n",
    "        batch_number = int(i / size_batch) + 1\n",
    "        testing_summaries_batch = []\n",
    "        for j in range(i, i + size_batch, size_subgroup):\n",
    "            testing_summaries_batch.append(window_summary(testing_features.iloc[j:j + size_subgroup]))\n",
    "        batch_mean = np.mean(testing_summaries_batch)\n",
    "        testing_summaries.append(np.mean(testing_summaries_batch))\n",
    "    total_batches = int(len(testing_features) / size_batch)\n",
    "    for i in range(len(testing_summaries)):\n",
    "        batch_number = i + 1\n",
    "        summary = testing_summaries[i]\n",
    "        if(summary < LCL_Ed or summary > UCL_Ed):\n",
    "            detected_batches.append(batch_number)\n",
    "            if (detected is False):\n",
    "                print('detected!')\n",
    "                detected = True\n",
    "            \n",
    "    if (detected is False):\n",
    "        print('no concept drift detected')\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    indices = np.arange(1, total_batches + 1)\n",
    "    plt.axhline(y = UCL_Ed, label='UCL', color='red')\n",
    "    plt.axhline(y = training_mean, label='training mean', color='green')\n",
    "    plt.axhline(y = LCL_Ed, label='LCL', color='yellow')\n",
    "    colors = []\n",
    "    for summary in testing_summaries:\n",
    "        if(summary < LCL_Ed or summary > UCL_Ed):\n",
    "            colors.append('red')\n",
    "        else:\n",
    "            colors.append('green')\n",
    "    ax.bar(indices, np.array(testing_summaries), color=colors)\n",
    "    plt.ylim(0.9 * LCL_Ed, 1.1 * UCL_Ed)\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "    print()\n",
    "    \n",
    "    return detected_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores_training_set_sqsi(classifier, training_features, training_labels):\n",
    "    countEvents = len(training_features)\n",
    "    cv = KFold(n_splits=20, shuffle=True)\n",
    "\n",
    "    scores_training_set= cross_val_predict(classifier, training_features, training_labels, cv=cv, n_jobs=-1, method='predict_proba')\n",
    "    return scores_training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqsi_all_batches(training_data, testing_features, size_batch, encoder, scaler):\n",
    "    training_features = training_data[0]\n",
    "    training_labels = training_data[1]\n",
    "    size_training = len(training_features)\n",
    "    all_features = training_features.append(testing_features)\n",
    "    all_features_ready = prepare_features(all_features, training_labels, size_training, encoder, scaler)\n",
    "    training_features = all_features_ready.iloc[0:size_training, :]\n",
    "    testing_features = all_features_ready.iloc[size_training:,]\n",
    "    \n",
    "    classifier = learn_classifier(training_features, training_labels)\n",
    "    scores_training_set = compute_scores_training_set_sqsi(classifier, training_features, training_labels)\n",
    "    detected_batches = []\n",
    "    detected = False\n",
    "    for i in range(0, len(testing_features), size_batch):\n",
    "        batch = testing_features[i:i + size_batch]\n",
    "        predict_probs_batch = classifier.predict_proba(batch)\n",
    "        probs_batch = predict_probs_batch[:,1]\n",
    "        probs_training = scores_training_set[:,1]\n",
    "        p_value = stats.ks_2samp(probs_batch,probs_training)[1]\n",
    "        batch_number = int((i) / size_batch + 1)\n",
    "        if(p_value < 0.001):\n",
    "            if (detected is False):\n",
    "                print('detected!')\n",
    "                detected = True\n",
    "            detected_batches.append(batch_number)\n",
    "    if (detected is False):\n",
    "        print('no concept drift detected')\n",
    "    total_batches = int(len(testing_features) / size_batch)\n",
    "    fig, ax = plt.subplots()\n",
    "    indices = np.arange(1, total_batches + 1)\n",
    "    colors = []\n",
    "    for i in indices:\n",
    "        if(i in detected_batches ):\n",
    "            colors.append('red')\n",
    "        else:\n",
    "            colors.append('green')\n",
    "    ax.bar(indices, np.ones(total_batches), color=colors)\n",
    "    plt.show()\n",
    "    print()\n",
    "    \n",
    "    return detected_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqsi_one_batch(training_data, testing_features, encoder, scaler):\n",
    "    training_features = training_data[0]\n",
    "    training_labels = training_data[1]\n",
    "    size_training = len(training_features)\n",
    "    all_features = training_features.append(testing_features)\n",
    "    all_features_ready = prepare_features(all_features, training_labels, size_training, encoder, scaler)\n",
    "    training_features = all_features_ready.iloc[0:size_training, :]\n",
    "    testing_features = all_features_ready.iloc[size_training:,]\n",
    "    \n",
    "    classifier = learn_classifier(training_features, training_labels)\n",
    "    scores_training_set = compute_scores_training_set_sqsi(classifier, training_features, training_labels)\n",
    "    detected = False\n",
    "\n",
    "    predict_probs_batch = classifier.predict_proba(testing_features)\n",
    "    probs_batch = predict_probs_batch[:,1]\n",
    "    probs_training = scores_training_set[:,1]\n",
    "    p_value = stats.ks_2samp(probs_batch,probs_training)[1]\n",
    "    if(p_value < 0.001):\n",
    "        print('detected!')\n",
    "        detected = True\n",
    "        \n",
    "    if (detected is False):\n",
    "        print('no concept drift detected')\n",
    "    \n",
    "    return detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label-dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_dependent_check_drift(training_data, testing_features, labels, size_batch, encoder, scaler, detector):\n",
    "    training_features = training_data[0]\n",
    "    training_labels = training_data[1]\n",
    "    size_training = len(training_features)\n",
    "    all_features = training_features.append(testing_features)\n",
    "    all_features_ready = prepare_features(all_features, training_labels, size_training, encoder, scaler)\n",
    "    training_features = all_features_ready.iloc[0:size_training, :]\n",
    "    testing_features = all_features_ready.iloc[size_training:,]\n",
    "    \n",
    "    classifier = learn_classifier(training_features, training_labels)\n",
    "    detected_batches = []\n",
    "    for i in range(0, len(testing_features), size_batch):\n",
    "        detected = False\n",
    "        batch = testing_features[i:i + size_batch]\n",
    "        y_predicted = classifier.predict(batch)\n",
    "        y_true = labels[len(training_features) + i:len(training_features) + i + size_batch]\n",
    "        diff = np.array(np.absolute(y_predicted - y_true))\n",
    "        for j in range(0, len(diff)):\n",
    "            _ = detector.update(diff[j])\n",
    "            if detector.drift_detected:\n",
    "                detected = True\n",
    "        batch_number = int((i) / size_batch + 1)\n",
    "        if(detected):\n",
    "            detected_batches.append(batch_number)\n",
    "            \n",
    "    total_batches = int(len(testing_features) / size_batch)\n",
    "    fig, ax = plt.subplots()\n",
    "    indices = np.arange(1, total_batches + 1)\n",
    "    colors = []\n",
    "    for i in indices:\n",
    "        if(i in detected_batches ):\n",
    "            colors.append('red')\n",
    "        else:\n",
    "            colors.append('green')\n",
    "    ax.bar(indices, np.ones(total_batches), color=colors)\n",
    "    plt.show()\n",
    "    print()\n",
    "    \n",
    "    return detected_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_runs_synthetic(actual_batch, predicted_batches, size_testing_batches):\n",
    "    size = len(predicted_batches)\n",
    "    latency = np.zeros(size)\n",
    "    false_positives_rates = np.zeros(size)\n",
    "    print('predicted batches:', predicted_batches)\n",
    "    \n",
    "    for i in range(size):\n",
    "        current_run = predicted_batches[i]\n",
    "        current_latency = 1\n",
    "        current_false_positives = 0\n",
    "        latency_found = False\n",
    "        for j in range(len(current_run)):\n",
    "            if(current_run[j] < actual_batch):\n",
    "                current_false_positives += 1\n",
    "            if(latency_found is False and current_run[j] < actual_batch):\n",
    "                current_latency = 0\n",
    "                latency_found = True\n",
    "            if(latency_found is False and current_run[j] >= actual_batch):\n",
    "                current_latency = (current_run[j] - actual_batch) / (size_testing_batches - actual_batch + 1)\n",
    "                latency_found = True\n",
    "                \n",
    "        false_positives_rates[i] = current_false_positives / (actual_batch - 1)\n",
    "        latency[i] = current_latency\n",
    "    print(\"latency:\", np.mean(latency))\n",
    "    print(\"false positives rate:\", np.mean(false_positives_rates))\n",
    "    return [np.mean(latency), np.mean(false_positives_rates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests_synthetic(path, label, size_dataset, size_training, size_batch, drift_start):\n",
    "    start = time.time()\n",
    "    print('dataset:', path)\n",
    "    #adjust size of data set so that testing batches are equal in size\n",
    "    size_dataset = size_training + int((size_dataset - size_training)/ size_batch) * size_batch\n",
    "    print('size dataset: ' + str(size_dataset) + ', size training: ' + str(size_training) + ', size testing batch: ' + str(size_batch))\n",
    "\n",
    "    actual_batch = int((drift_start - size_training) / size_batch) + 1\n",
    "    print('actual concept drift is in batch', actual_batch)    \n",
    "    print()\n",
    "    predicted_batches = []\n",
    "    number_testing_batches = (size_dataset - size_training) / size_batch\n",
    "    features, labels = read_dataset(path, label)\n",
    "    \n",
    "    training_data = get_training_data(features, labels, size_training)\n",
    "    testing_features = features.iloc[size_training:size_dataset, :]\n",
    "    \n",
    "    #run SQSI detector on the synthetic data sets\n",
    "    print(\"SQSI\")\n",
    "    encoders =  [None, OrdinalEncoder(), TargetEncoder(), OneHotEncoder()]\n",
    "    scalers = [False, True]\n",
    "    if ('sea_1' in path):\n",
    "        encoders = [None]\n",
    "        \n",
    "    if('gradual' in path):\n",
    "        encoders = [OrdinalEncoder()]\n",
    "        \n",
    "    for encoder in encoders:\n",
    "        for scaler in scalers:\n",
    "            sqsi_drifted_batches = sqsi_all_batches(training_data, testing_features, size_batch, encoder, scaler)\n",
    "            predicted_batches.append(sqsi_drifted_batches)\n",
    "            latency, fpr_s = evaluate_runs_synthetic(actual_batch, [sqsi_drifted_batches], number_testing_batches)\n",
    "            print('latency', format(latency, '.8f'))\n",
    "            print('false positive rate', format(fpr_s, '.8f'))\n",
    "            print()\n",
    "            \n",
    "    #run UDETECT detector on the synthetic data sets\n",
    "    print(\"UDetect\")\n",
    "    encoders =  [None, OrdinalEncoder(), TargetEncoder(), OneHotEncoder()]\n",
    "    scalers = [True]\n",
    "    if ('sea_1' in path):\n",
    "        encoders = [None]\n",
    "        scalers = [False]\n",
    "\n",
    "    if('gradual' in path):\n",
    "        encoders = [OrdinalEncoder()]\n",
    "        scalers = [False]\n",
    "        \n",
    "    for encoder in encoders:\n",
    "        for scaler in scalers:\n",
    "            sqsi_drifted_batches = udetect_all_batches(training_data, testing_features, size_batch, encoder, scaler)\n",
    "            predicted_batches.append(sqsi_drifted_batches)\n",
    "            latency, fpr_s = evaluate_runs_synthetic(actual_batch, [sqsi_drifted_batches], number_testing_batches)\n",
    "\n",
    "            print()\n",
    "    \n",
    "#     average_latency, average_fpr_s = evaluate_runs_synthetic(actual_batch, predicted_batches, number_testing_batches)\n",
    "#     print('-----')\n",
    "#     print('metrics based on all runs for the data set')\n",
    "#     print('average Latency', format(average_latency, '.8f'))\n",
    "#     print('average FPR_S', format(average_fpr_s, '.8f'))\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"duration of test: \" + str(int((end - start) / 60)) + ' minutes')\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run tests on synthetic data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['synthetic/abrupt_drift/sea_1_abrupt_drift_0_noise_balanced.arff',\n",
    "            'synthetic/abrupt_drift/agraw1_1_abrupt_drift_0_noise_balanced.arff',\n",
    "            'synthetic/abrupt_drift/agraw2_1_abrupt_drift_0_noise_balanced.arff',\n",
    "            'synthetic/gradual_drift/sea_1_gradual_drift_0_noise_balanced_05.arff',\n",
    "            'synthetic/gradual_drift/sea_1_gradual_drift_0_noise_balanced_1.arff',\n",
    "            'synthetic/gradual_drift/sea_1_gradual_drift_0_noise_balanced_5.arff',\n",
    "            'synthetic/gradual_drift/sea_1_gradual_drift_0_noise_balanced_10.arff',\n",
    "            'synthetic/gradual_drift/sea_1_gradual_drift_0_noise_balanced_20.arff',\n",
    "            'synthetic/gradual_drift/agraw2_1_gradual_drift_0_noise_balanced_05.arff',\n",
    "            'synthetic/gradual_drift/agraw2_1_gradual_drift_0_noise_balanced_1.arff',\n",
    "            'synthetic/gradual_drift/agraw2_1_gradual_drift_0_noise_balanced_5.arff',\n",
    "            'synthetic/gradual_drift/agraw2_1_gradual_drift_0_noise_balanced_10.arff',\n",
    "            'synthetic/gradual_drift/agraw2_1_gradual_drift_0_noise_balanced_20.arff',\n",
    "            'synthetic/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_05.arff',\n",
    "            'synthetic/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_1.arff',\n",
    "            'synthetic/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_5.arff',\n",
    "            'synthetic/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_10.arff',\n",
    "            'synthetic/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_20.arff',\n",
    "           ]\n",
    "length_datasets = len(datasets)\n",
    "for i in range(length_datasets):\n",
    "    run_tests_synthetic(datasets[i], 'class', 100000, 30000, 10000, 55000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-world data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_runs_real_world(reference_drifted_batches, predicted_batches, number_batches):\n",
    "    size = len(predicted_batches)\n",
    "    drift_detection_rates = np.zeros(size)\n",
    "    false_positives_rates = np.zeros(size)\n",
    "    print('predicted batches:', predicted_batches)\n",
    "    print('number batches', number_batches)\n",
    "    \n",
    "    for i in range(size):\n",
    "        current_run = predicted_batches[i]\n",
    "        correctly_detected = 0\n",
    "        incorrectly_detected = 0\n",
    "        for j in range(len(current_run)):\n",
    "            if(current_run[j] in reference_drifted_batches):\n",
    "                correctly_detected += 1\n",
    "            else:\n",
    "                incorrectly_detected += 1\n",
    "        if(len(reference_drifted_batches) == 0):\n",
    "            drift_detection_rates[i] = 0\n",
    "        else:\n",
    "            drift_detection_rates[i] = correctly_detected / len(reference_drifted_batches)\n",
    "        false_positives_rates[i] = incorrectly_detected / (number_batches - len(reference_drifted_batches))\n",
    "      \n",
    "    DDR = np.mean(drift_detection_rates)\n",
    "    FPR_R = np.mean(false_positives_rates)\n",
    "    print(\"DDR:\", DDR)\n",
    "    print(\"FPR_R:\", FPR_R)\n",
    "    return [FPR_R, DDR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests_real_world(path, label, size_dataset, size_training, size_batch):\n",
    "    start = time.time()\n",
    "    print('dataset:', path)\n",
    "    #adjust size of data set so that testing batches are equal in size\n",
    "    size_dataset = size_training + int((size_dataset - size_training)/ size_batch) * size_batch\n",
    "    print('size dataset: ' + str(size_dataset) + ', size training: ' + str(size_training) + ', size testing batch: ' + str(size_batch))\n",
    "\n",
    "    print()\n",
    "    predicted_batches = []\n",
    "    number_testing_batches = (size_dataset - size_training) / size_batch\n",
    "    features, labels = read_dataset(path, label)\n",
    "    \n",
    "    training_data = get_training_data(features, labels, size_training)\n",
    "    testing_features = features.iloc[size_training:size_dataset, :]\n",
    "    reference_drifted_batches_seq = []\n",
    "    reference_drifted_batches_time = []\n",
    "    number_testing_batches = (size_dataset - size_training) / size_batch\n",
    "    \n",
    "    scalers = []\n",
    "    encoders = []\n",
    "    \n",
    "    if('spam' in path):\n",
    "        scalers = [False]\n",
    "        encoders = [None]\n",
    "        if(size_batch == 100):\n",
    "            reference_drifted_batches_seq  = [7, 8, 10, 11, 12, 13, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
    "            reference_drifted_batches_time = [3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
    "        if(size_batch == 50):\n",
    "            reference_drifted_batches_seq  = [11, 14, 15, 16, 17, 19, 20, 21, 22, 23, 25, 27, 29, 30, 31, 32, 33, 34, 35, 37, 40, 41, 42, 44, 45, 46, 47, 48, 50, 51, 52, 53, 55, 56, 57, 58]\n",
    "            reference_drifted_batches_time = [1, 3, 4, 5, 7, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58]\n",
    "        if(size_batch == 20): \n",
    "            reference_drifted_batches_seq  = [10, 14, 21, 26, 32, 33, 35, 36, 37, 39, 40, 41, 42, 47, 49, 50, 51, 52, 53, 54, 56, 57, 58, 60, 61, 62, 63, 67, 68, 72, 73, 76, 78, 80, 81, 82, 83, 85, 86, 87, 88, 91, 92, 93, 95, 97, 98, 99, 102, 103, 104, 105, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 130, 131, 132, 135, 136, 137, 138, 139, 141, 142, 143, 145]\n",
    "            reference_drifted_batches_time = [2, 10, 11, 14, 17, 21, 22, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 70, 72, 73, 74, 76, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 90, 92, 93, 95, 97, 98, 99, 102, 103, 104, 105, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145]\n",
    "            \n",
    "    if('weather' in path):\n",
    "        scalers = [True]\n",
    "        encoders = [None]\n",
    "        if(size_batch == 365):    \n",
    "            reference_drifted_batches_seq  = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33]\n",
    "            reference_drifted_batches_time = [2, 3, 4, 6, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 26, 28, 29, 30, 32, 33]\n",
    "        if(size_batch == 30):\n",
    "            reference_drifted_batches_seq  = [6, 8, 11, 12, 16, 17, 18, 19, 20, 21, 27, 28, 29, 30, 31, 32, 39, 41, 42, 43, 44, 49, 50, 51, 52, 53, 54, 55, 58, 61, 62, 63, 64, 65, 66, 67, 68, 69, 73, 74, 75, 76, 77, 78, 79, 80, 81, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 236, 239, 245, 247, 248, 249, 250, 251, 254, 262, 263, 264, 265, 266, 269, 271, 272, 273, 275, 276, 277, 278, 279, 281, 283, 284, 285, 286, 287, 288, 290, 291, 292, 297, 299, 302, 305, 306, 307, 308, 309, 311, 312, 313, 314, 315, 316, 319, 322, 324, 330, 332, 333, 334, 335, 336, 338, 339, 340, 342, 343, 346, 347, 350, 351, 352, 356, 357, 358, 359, 360, 361, 362, 363, 367, 372, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 396, 397, 399, 400, 403]\n",
    "            reference_drifted_batches_time = [1, 10, 11, 12, 13, 14, 18, 24, 25, 26, 27, 28, 29, 33, 35, 36, 38, 43, 44, 45, 46, 47, 48, 49, 50, 56, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 76, 79, 80, 81, 83, 84, 85, 92, 95, 96, 106, 107, 108, 117, 125, 126, 127, 129, 130, 132, 133, 134, 135, 140, 141, 143, 144, 145, 146, 147, 148, 149, 150, 152, 153, 154, 155, 156, 157, 160, 161, 162, 164, 165, 166, 167, 172, 175, 176, 177, 178, 179, 180, 182, 185, 186, 188, 189, 190, 191, 192, 194, 195, 196, 199, 200, 201, 202, 203, 205, 206, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 233, 235, 238, 239, 244, 246, 247, 248, 249, 250, 251, 254, 255, 256, 257, 259, 260, 261, 262, 263, 264, 265, 266, 267, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 295, 297, 298, 299, 302, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 319, 320, 322, 324, 325, 330, 332, 333, 334, 335, 336, 337, 338, 339, 340, 342, 343, 346, 347, 349, 350, 351, 352, 355, 356, 357, 358, 359, 360, 361, 362, 363, 367, 372, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 391, 395, 396, 397, 399, 400, 401, 402, 403]\n",
    "       \n",
    "    if('elect' in path):\n",
    "        scalers = [True]\n",
    "        encoders = [None]\n",
    "        if(size_batch == 365):\n",
    "            reference_drifted_batches_seq = [2, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 34, 35, 36, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82]\n",
    "            reference_drifted_batches_time = [2, 3, 4, 5, 6, 8, 9, 10, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 34, 35, 36, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82]\n",
    "    \n",
    "    if('airline' in path):\n",
    "        scalers = [True]\n",
    "        encoders = [OrdinalEncoder(), OneHotEncoder(), TargetEncoder()]\n",
    "        reference_drifted_batches_seq  = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
    "        reference_drifted_batches_time = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
    "\n",
    "    # #run UDETECT detector on the real-world data sets\n",
    "    print(\"UDetect\")\n",
    "    for encoder in encoders:\n",
    "        for scaler in scalers:\n",
    "            if(isinstance(encoder, OneHotEncoder)):\n",
    "                reference_drifted_batches_seq = [2,3,5,6,7,8,9,10,11,12,13,15,16,17,18,19,20,21]\n",
    "            if(isinstance(encoder, OrdinalEncoder)):\n",
    "                reference_drifted_batches_seq = [2,3,4,5,6,7,8,9,10,11,12,13,15,16,17,18,19,20,21]\n",
    "            if(isinstance(encoder, TargetEncoder)):\n",
    "                reference_drifted_batches_seq = [2]\n",
    "                \n",
    "        udetect_drifted_batches = udetect_all_batches(training_data, testing_features, size_batch, encoder, scaler)\n",
    "        predicted_batches.append(udetect_drifted_batches)\n",
    "        print('sequential split')\n",
    "        FPR_R_seq, DDR_seq = evaluate_runs_real_world(reference_drifted_batches_seq, [udetect_drifted_batches], number_testing_batches)\n",
    "        if('airline' not in path):\n",
    "            print('time-based split')\n",
    "            FPR_R_time, DDR_time = evaluate_runs_real_world(reference_drifted_batches_time, [udetect_drifted_batches], number_testing_batches)\n",
    "        print()\n",
    "    \n",
    "#     average_fpr_seq_split, average_ddr_seq_split = evaluate_runs_real_world(reference_drifted_batches_seq, predicted_batches, number_testing_batches)\n",
    "#     average_fpr_time_split, average_ddr_time_split = evaluate_runs_real_world(reference_drifted_batches_time, predicted_batches, number_testing_batches)\n",
    "#     print('-----')\n",
    "#     print('metrics based on all runs for the data set')\n",
    "#     print('average FPR_R', format((average_fpr_seq_split + average_fpr_time_split) / 2, '.8f'))\n",
    "#     print('average DDR', format((average_ddr_seq_split + average_ddr_time_split) / 2, '.8f'))\n",
    "\n",
    "#     #run SQSI detector on the real-world data sets\n",
    "    print(\"SQSI\")\n",
    "    for encoder in encoders:\n",
    "        for scaler in scalers:\n",
    "            if(isinstance(encoder, OneHotEncoder)):\n",
    "                reference_drifted_batches_seq = [2,3,5,6,7,8,9,10,11,12,13,15,16,17,18,19,20,21]\n",
    "            if(isinstance(encoder, OrdinalEncoder)):\n",
    "                reference_drifted_batches_seq = [2,3,4,5,6,7,8,9,10,11,12,13,15,16,17,18,19,20,21]\n",
    "            if(isinstance(encoder, TargetEncoder)):\n",
    "                reference_drifted_batches_seq = [2]\n",
    "                \n",
    "        sqsi_drifted_batches = sqsi_all_batches(training_data, testing_features, size_batch, encoder, scaler)\n",
    "        predicted_batches.append(sqsi_drifted_batches)\n",
    "        print('sequential split')\n",
    "        FPR_R_seq, DDR_seq = evaluate_runs_real_world(reference_drifted_batches_seq, [sqsi_drifted_batches], number_testing_batches)\n",
    "        if('airline' not in path):\n",
    "            print('time-based split')\n",
    "            FPR_R_time, DDR_time = evaluate_runs_real_world(reference_drifted_batches_time, [sqsi_drifted_batches], number_testing_batches)\n",
    "        print()\n",
    "    \n",
    "#     average_fpr_seq_split, average_ddr_seq_split = evaluate_runs_real_world(reference_drifted_batches_seq, predicted_batches, number_testing_batches)\n",
    "#     average_fpr_time_split, average_ddr_time_split = evaluate_runs_real_world(reference_drifted_batches_time, predicted_batches, number_testing_batches)\n",
    "#     print('-----')\n",
    "#     print('metrics based on all runs for the data set')\n",
    "#     print('average FPR_R', format((average_fpr_seq_split + average_fpr_time_split) / 2, '.8f'))\n",
    "#     print('average DDR', format((average_ddr_seq_split + average_ddr_time_split) / 2, '.8f'))\n",
    "        \n",
    "    #run label-dependent-detectors on the real-world data sets  \n",
    "    print(\"label-dependent detectors\")\n",
    "    detectors = [drift.EDDM(), drift.DDM(), drift.ADWIN(), drift.HDDM_A(), drift.HDDM_W()]\n",
    "    for encoder in encoders:\n",
    "        for scaler in scalers:\n",
    "            for detector in detectors:\n",
    "                print(detector.__class__.__name__)\n",
    "                if('airline' in path and size_batch == 17000): \n",
    "                    if(isinstance(encoder, OneHotEncoder)):\n",
    "                        reference_drifted_batches_seq = [2,3,5,6,7,8,9,10,11,12,13,15,16,17,18,19,20,21]\n",
    "                    if(isinstance(encoder, OrdinalEncoder)):\n",
    "                        reference_drifted_batches_seq = [2,3,4,5,6,7,8,9,10,11,12,13,15,16,17,18,19,20,21]\n",
    "                    if(isinstance(encoder, TargetEncoder)):\n",
    "                        reference_drifted_batches_seq = [2]\n",
    "                    \n",
    "                label_dependent_batches = label_dependent_check_drift(training_data, testing_features, labels, size_batch, encoder, scaler, detector)\n",
    "                predicted_batches.append(label_dependent_batches)\n",
    "                print('sequential split')\n",
    "                FPR_R_seq, DDR_seq = evaluate_runs_real_world(reference_drifted_batches_seq, [label_dependent_batches], number_testing_batches)\n",
    "                if('airline' not in path):\n",
    "                    print('time-based split')\n",
    "                    FPR_R_time, DDR_time = evaluate_runs_real_world(reference_drifted_batches_time, [label_dependent_batches], number_testing_batches)\n",
    "                print()\n",
    "    \n",
    "#     average_fpr_seq_split, average_ddr_seq_split = evaluate_runs_real_world(reference_drifted_batches_seq, predicted_batches, number_testing_batches)\n",
    "#     average_fpr_time_split, average_ddr_time_split = evaluate_runs_real_world(reference_drifted_batches_time, predicted_batches, number_testing_batches)\n",
    "    \n",
    "#     print('-----')\n",
    "#     print('metrics based on all runs for the data set')\n",
    "#     print('average FPR_R', format((average_fpr_seq_split + average_fpr_time_split) / 2, '.8f'))\n",
    "#     print('average DDR', format((average_ddr_seq_split + average_ddr_time_split) / 2, '.8f'))\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"duration of test: \" + str(int((end - start) / 60)) + ' minutes')\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run tests on real-world data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_tests_real_world('real-world/electricity_dataset.csv', 'label', 45312, 15104, 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_tests_real_world('real-world/weather_dataset.csv', 'Label_Rain', 18159, 6053, 365)\n",
    "run_tests_real_world('real-world/weather_dataset.csv', 'Label_Rain', 18159, 6053, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_tests_real_world('real-world/spam_dataset.csv', 'ACTUAL_LABEL', 4405, 1468, 100)\n",
    "run_tests_real_world('real-world/spam_dataset.csv', 'ACTUAL_LABEL', 4405, 1468, 20)\n",
    "run_tests_real_world('real-world/spam_dataset.csv', 'ACTUAL_LABEL', 4405, 1468, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_tests_real_world('real-world/airline_dataset.csv','Delay', 539383, 179794, 17000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
